{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1b1Q86sb3-87IYqynkcQfsc42hGkvlN_f",
      "authorship_tag": "ABX9TyPsyiRwaLgKpATlZ5tq3Rex",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qm1ne/GameDesignBalance/blob/main/%20Q-Learningmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stable-baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN_S_2kupbML",
        "outputId": "763700c2-347f-4bd4-bb98-680c88648f20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.7.0-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install gymnasium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEOHDJRGrlSr",
        "outputId": "f796244b-9175-44a4-a9a9-5db871efe055"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import torch\n",
        "\n",
        "\n",
        "def create_deck():\n",
        "    suits = ['♠', '♥', '♦', '♣']\n",
        "    ranks = list(range(1, 8)) + ['J', 'Q', 'K']\n",
        "    return [f\"{r}{s}\" for s in suits for r in ranks]\n",
        "\n",
        "\n",
        "def card_value(card):\n",
        "    r = card[:-1]\n",
        "    if r == 'J': return 8\n",
        "    if r == 'Q': return 9\n",
        "    if r == 'K': return 10\n",
        "    return int(r)\n",
        "\n",
        "\n",
        "def card_to_id(card):\n",
        "    \"\"\"Convertit une carte en ID numérique pour l'IA\"\"\"\n",
        "    suits = ['♠', '♥', '♦', '♣']\n",
        "    ranks = list(range(1, 8)) + ['J', 'Q', 'K']\n",
        "\n",
        "    suit = card[-1]\n",
        "    rank = card[:-1]\n",
        "\n",
        "    try:\n",
        "        suit_id = suits.index(suit)\n",
        "        if rank.isdigit():\n",
        "            rank_id = ranks.index(int(rank))\n",
        "        else:\n",
        "            rank_id = ranks.index(rank)\n",
        "\n",
        "        return suit_id * len(ranks) + rank_id\n",
        "    except (ValueError, IndexError):\n",
        "        return 0\n",
        "\n",
        "\n",
        "def id_to_card(card_id):\n",
        "    \"\"\"Convertit un ID numérique en carte\"\"\"\n",
        "    suits = ['♠', '♥', '♦', '♣']\n",
        "    ranks = list(range(1, 8)) + ['J', 'Q', 'K']\n",
        "\n",
        "    if card_id >= len(suits) * len(ranks) or card_id < 0:\n",
        "        return \"1♠\"\n",
        "\n",
        "    suit_id = card_id // len(ranks)\n",
        "    rank_id = card_id % len(ranks)\n",
        "\n",
        "    if suit_id >= len(suits) or rank_id >= len(ranks):\n",
        "        return \"1♠\"\n",
        "\n",
        "    return f\"{ranks[rank_id]}{suits[suit_id]}\"\n",
        "\n",
        "\n",
        "class CardGameEnv(gym.Env):\n",
        "    \"\"\"Environnement Gymnasium simplifié pour Q-Learning\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CardGameEnv, self).__init__()\n",
        "\n",
        "        # Actions: jouer une carte (0-43) ou utiliser une capacité (44-47)\n",
        "        self.action_space = spaces.Discrete(48)\n",
        "\n",
        "        # Observation: état du jeu simplifié\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=1, shape=(100,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.deck = create_deck()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        shuffled_deck = self.deck.copy()\n",
        "        random.shuffle(shuffled_deck)\n",
        "\n",
        "        self.table = shuffled_deck[:4]\n",
        "        self.player_hand = shuffled_deck[4:7]\n",
        "        self.opponent_hand = shuffled_deck[7:10]\n",
        "        self.remaining_deck = shuffled_deck[10:]\n",
        "\n",
        "        self.player_captured = []\n",
        "        self.opponent_captured = []\n",
        "        self.turn = 0\n",
        "        self.game_over = False\n",
        "\n",
        "        # Capacités - simplement disponibles ou non\n",
        "        self.player_abilities_used = [False, False, False, False]\n",
        "\n",
        "        return self._get_observation(), {}\n",
        "\n",
        "    def _get_observation(self):\n",
        "        \"\"\"État du jeu pour l'agent Q-Learning\"\"\"\n",
        "        obs = np.zeros(100, dtype=np.float32)\n",
        "\n",
        "        # Main du joueur (44 positions)\n",
        "        for card in self.player_hand:\n",
        "            card_id = card_to_id(card)\n",
        "            if 0 <= card_id < 44:\n",
        "                obs[card_id] = 1.0\n",
        "\n",
        "        # Table (44 positions)\n",
        "        for card in self.table:\n",
        "            card_id = card_to_id(card)\n",
        "            if 0 <= card_id < 44:\n",
        "                obs[44 + card_id] = 1.0\n",
        "\n",
        "        # Informations de base\n",
        "        obs[88] = len(self.player_hand) / 10.0\n",
        "        obs[89] = len(self.table) / 10.0\n",
        "        obs[90] = len(self.player_captured) / 20.0\n",
        "        obs[91] = self.turn / 100.0\n",
        "\n",
        "        # Capacités disponibles\n",
        "        for i, used in enumerate(self.player_abilities_used):\n",
        "            obs[92 + i] = 0.0 if used else 1.0\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.game_over:\n",
        "            return self._get_observation(), 0, True, False, {}\n",
        "\n",
        "        reward = 0\n",
        "\n",
        "        # Actions de cartes (0-43)\n",
        "        if action < 44:\n",
        "            reward = self._play_card_action(action)\n",
        "        # Actions de capacités (44-47)\n",
        "        else:\n",
        "            ability_id = action - 44\n",
        "            reward = self._use_ability_action(ability_id)\n",
        "\n",
        "        # Tour de l'adversaire\n",
        "        if not self.game_over:\n",
        "            self._opponent_turn()\n",
        "\n",
        "        # Vérifier fin de partie\n",
        "        if not self.player_hand and not self.opponent_hand and not self.remaining_deck:\n",
        "            self.game_over = True\n",
        "            winner = self._determine_winner()\n",
        "            if winner == \"win\":\n",
        "                reward += 10.0\n",
        "            elif winner == \"lose\":\n",
        "                reward -= 10.0\n",
        "            # draw = 0 reward\n",
        "\n",
        "        self._distribute_cards()\n",
        "        self.turn += 1\n",
        "\n",
        "        return self._get_observation(), reward, self.game_over, False, {}\n",
        "\n",
        "    def _play_card_action(self, card_id):\n",
        "        \"\"\"Joue une carte - l'agent apprend quoi faire\"\"\"\n",
        "        if card_id >= 44 or card_id < 0:\n",
        "            return -1.0\n",
        "\n",
        "        card = id_to_card(card_id)\n",
        "\n",
        "        if card not in self.player_hand:\n",
        "            return -1.0  # Action invalide\n",
        "\n",
        "        cv = card_value(card)\n",
        "\n",
        "        # Chercher une capture possible\n",
        "        captured = False\n",
        "        for table_card in self.table[:]:\n",
        "            if card_value(table_card) == cv:\n",
        "                self.player_hand.remove(card)\n",
        "                self.table.remove(table_card)\n",
        "                self.player_captured.extend([card, table_card])\n",
        "\n",
        "                # Récompense pour capture\n",
        "                reward = 2.0\n",
        "\n",
        "                # Bonus si table vide\n",
        "                if not self.table:\n",
        "                    reward += 3.0\n",
        "\n",
        "                captured = True\n",
        "                break\n",
        "\n",
        "        if not captured:\n",
        "            # Placer sur la table\n",
        "            self.player_hand.remove(card)\n",
        "            self.table.append(card)\n",
        "            reward = -0.1\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def _use_ability_action(self, ability_id):\n",
        "        \"\"\"Utilise une capacité - l'agent apprend quand c'est utile\"\"\"\n",
        "        if ability_id >= 4 or self.player_abilities_used[ability_id]:\n",
        "            return -1.0  # Capacité non disponible\n",
        "\n",
        "        # Marquer comme utilisée\n",
        "        self.player_abilities_used[ability_id] = True\n",
        "\n",
        "        # L'agent doit apprendre quand utiliser chaque capacité\n",
        "        # Pas de logique prédéfinie - juste des récompenses de base\n",
        "\n",
        "        if ability_id == 0:  # swap_opponent\n",
        "            if self.opponent_hand and self.player_hand:\n",
        "                # Échange aléatoire - l'agent apprend si c'est bon\n",
        "                opp_card = random.choice(self.opponent_hand)\n",
        "                my_card = random.choice(self.player_hand)\n",
        "\n",
        "                self.opponent_hand.remove(opp_card)\n",
        "                self.player_hand.remove(my_card)\n",
        "                self.opponent_hand.append(my_card)\n",
        "                self.player_hand.append(opp_card)\n",
        "\n",
        "                return 1.0  # Récompense neutre\n",
        "            return -0.5\n",
        "\n",
        "        elif ability_id == 1:  # swap_table\n",
        "            if self.table and self.player_hand:\n",
        "                table_card = random.choice(self.table)\n",
        "                my_card = random.choice(self.player_hand)\n",
        "\n",
        "                self.table.remove(table_card)\n",
        "                self.player_hand.remove(my_card)\n",
        "                self.table.append(my_card)\n",
        "                self.player_hand.append(table_card)\n",
        "\n",
        "                return 1.0\n",
        "            return -0.5\n",
        "\n",
        "        elif ability_id == 2:  # reveal\n",
        "            # Pas d'effet mécanique - juste une récompense d'information\n",
        "            return 0.5\n",
        "\n",
        "        elif ability_id == 3:  # bonus7\n",
        "            # Bonus basé sur les 7 dans la main\n",
        "            sevens = sum(1 for card in self.player_hand if card_value(card) == 7)\n",
        "            return sevens * 0.5\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def _opponent_turn(self):\n",
        "        \"\"\"Adversaire avec stratégie basique\"\"\"\n",
        "        if not self.opponent_hand:\n",
        "            return\n",
        "\n",
        "        # Stratégie simple: capturer si possible, sinon jouer première carte\n",
        "        played = False\n",
        "        for card in self.opponent_hand[:]:\n",
        "            cv = card_value(card)\n",
        "            for table_card in self.table[:]:\n",
        "                if card_value(table_card) == cv:\n",
        "                    self.opponent_hand.remove(card)\n",
        "                    self.table.remove(table_card)\n",
        "                    self.opponent_captured.extend([card, table_card])\n",
        "                    played = True\n",
        "                    break\n",
        "            if played:\n",
        "                break\n",
        "\n",
        "        if not played and self.opponent_hand:\n",
        "            card = self.opponent_hand.pop(0)\n",
        "            self.table.append(card)\n",
        "\n",
        "    def _distribute_cards(self):\n",
        "        \"\"\"Distribue nouvelles cartes\"\"\"\n",
        "        while len(self.player_hand) < 3 and self.remaining_deck:\n",
        "            self.player_hand.append(self.remaining_deck.pop(0))\n",
        "\n",
        "        while len(self.opponent_hand) < 3 and self.remaining_deck:\n",
        "            self.opponent_hand.append(self.remaining_deck.pop(0))\n",
        "\n",
        "    def _determine_winner(self):\n",
        "        \"\"\"Détermine le gagnant - retourne string simple\"\"\"\n",
        "        player_points = 0\n",
        "        opponent_points = 0\n",
        "\n",
        "        # Plus de cartes capturées\n",
        "        if len(self.player_captured) > len(self.opponent_captured):\n",
        "            player_points += 1\n",
        "        elif len(self.opponent_captured) > len(self.player_captured):\n",
        "            opponent_points += 1\n",
        "\n",
        "        # Plus de 7\n",
        "        player_sevens = sum(1 for card in self.player_captured if card_value(card) == 7)\n",
        "        opponent_sevens = sum(1 for card in self.opponent_captured if card_value(card) == 7)\n",
        "\n",
        "        if player_sevens > opponent_sevens:\n",
        "            player_points += 1\n",
        "        elif opponent_sevens > player_sevens:\n",
        "            opponent_points += 1\n",
        "\n",
        "        if player_points > opponent_points:\n",
        "            return \"win\"\n",
        "        elif player_points < opponent_points:\n",
        "            return \"lose\"\n",
        "        else:\n",
        "            return \"draw\"\n",
        "\n",
        "\n",
        "class QLearningPlayer:\n",
        "    \"\"\"Joueur utilisant uniquement Q-Learning (DQN)\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.env = None\n",
        "\n",
        "    def train(self, total_timesteps=500):\n",
        "        \"\"\"Entraîne le modèle Q-Learning\"\"\"\n",
        "        print(f\"🧠 Entraînement Q-Learning...\")\n",
        "\n",
        "        self.env = CardGameEnv()\n",
        "\n",
        "        # Configuration DQN pour Q-Learning\n",
        "        self.model = DQN(\n",
        "            'MlpPolicy',\n",
        "            self.env,\n",
        "            verbose=1,\n",
        "            learning_rate=1,\n",
        "            buffer_size=100,\n",
        "            learning_starts=10,\n",
        "            batch_size=32,\n",
        "            tau=1.0,\n",
        "            gamma=0.99,\n",
        "            exploration_fraction=0.3,\n",
        "            exploration_initial_eps=1.0,\n",
        "            exploration_final_eps=0.1\n",
        "        )\n",
        "\n",
        "        self.model.learn(total_timesteps=total_timesteps)\n",
        "        print(f\"✅ Entraînement Q-Learning terminé!\")\n",
        "\n",
        "    def evaluate(self, n_games=50):\n",
        "        \"\"\"Évalue les performances\"\"\"\n",
        "        if not self.model:\n",
        "            print(\"❌ Modèle non entraîné!\")\n",
        "            return\n",
        "\n",
        "        print(f\"📊 Évaluation sur {n_games} parties...\")\n",
        "\n",
        "        mean_reward, std_reward = evaluate_policy(\n",
        "            self.model, self.env, n_eval_episodes=n_games\n",
        "        )\n",
        "\n",
        "        print(f\"Récompense moyenne: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
        "        return mean_reward, std_reward\n",
        "\n",
        "    def play_game(self):\n",
        "        \"\"\"Joue une partie et retourne le résultat\"\"\"\n",
        "        if not self.model:\n",
        "            return \"Modèle non entraîné\"\n",
        "\n",
        "        obs, _ = self.env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action, _ = self.model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _, _ = self.env.step(action)\n",
        "\n",
        "        return self.env._determine_winner()\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        \"\"\"Sauvegarde le modèle\"\"\"\n",
        "        if self.model:\n",
        "            self.model.save(filename)\n",
        "            print(f\"💾 Modèle Q-Learning sauvegardé: {filename}\")\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        \"\"\"Charge un modèle\"\"\"\n",
        "        self.model = DQN.load(filename)\n",
        "        print(f\"📂 Modèle Q-Learning chargé: {filename}\")\n",
        "\n",
        "\n",
        "def test_qlearning_performance(n_tests=100):\n",
        "    \"\"\"Test les performances du Q-Learning\"\"\"\n",
        "    print(f\"🎯 Test de performance Q-Learning sur {n_tests} parties\")\n",
        "\n",
        "    player = QLearningPlayer()\n",
        "    player.train(total_timesteps=200)\n",
        "\n",
        "    results = {\"win\": 0, \"lose\": 0, \"draw\": 0}\n",
        "\n",
        "    for i in range(n_tests):\n",
        "        result = player.play_game()\n",
        "        results[result] += 1\n",
        "\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f\"Partie {i+1}/{n_tests}: Victoires={results['win']}, Défaites={results['lose']}, Nuls={results['draw']}\")\n",
        "\n",
        "    print(\"\\n📊 RÉSULTATS FINAUX:\")\n",
        "    print(f\"Victoires: {results['win']}/{n_tests} ({results['win']/n_tests*100:.1f}%)\")\n",
        "    print(f\"Défaites: {results['lose']}/{n_tests} ({results['lose']/n_tests*100:.1f}%)\")\n",
        "    print(f\"Nuls: {results['draw']}/{n_tests} ({results['draw']/n_tests*100:.1f}%)\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🎮 Jeu de Cartes avec Q-Learning Pur\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Test rapide\n",
        "        print(\"\\n🧠 Entraînement Q-Learning...\")\n",
        "        player = QLearningPlayer()\n",
        "        player.train(total_timesteps=100)\n",
        "        player.evaluate(n_games=10)\n",
        "        player.save_model(\"qlearning_model\")\n",
        "\n",
        "        # Test de performance\n",
        "        print(\"\\n🎯 Test de performance...\")\n",
        "        performance = test_qlearning_performance(50)\n",
        "\n",
        "        print(\"\\n✅ Test Q-Learning réussi!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur: {e}\")\n",
        "        print(\"Vérifiez: pip install gymnasium stable-baselines3 torch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H42RAiJcxFHy",
        "outputId": "5e4ad703-7c2b-42ff-8dbe-332bdd4c3549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎮 Jeu de Cartes avec Q-Learning Pur\n",
            "==================================================\n",
            "\n",
            "🧠 Entraînement Q-Learning...\n",
            "🧠 Entraînement Q-Learning...\n",
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "✅ Entraînement Q-Learning terminé!\n",
            "📊 Évaluation sur 10 parties...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "\n",
        "def create_deck():\n",
        "    suits = ['♠', '♥', '♦', '♣']\n",
        "    ranks = list(range(1, 8)) + ['J', 'Q', 'K']\n",
        "    return [f\"{r}{s}\" for s in suits for r in ranks]\n",
        "\n",
        "\n",
        "def card_value(card):\n",
        "    r = card[:-1]\n",
        "    return {'J': 8, 'Q': 9, 'K': 10}.get(r, int(r))\n",
        "\n",
        "\n",
        "def card_to_id(card):\n",
        "    suits = ['♠', '♥', '♦', '♣']\n",
        "    ranks = list(range(1, 8)) + ['J', 'Q', 'K']\n",
        "    try:\n",
        "        suit_id = suits.index(card[-1])\n",
        "        rank = card[:-1]\n",
        "        rank_id = ranks.index(int(rank)) if rank.isdigit() else ranks.index(rank)\n",
        "        return suit_id * len(ranks) + rank_id\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def id_to_card(card_id):\n",
        "    suits = ['♠', '♥', '♦', '♣']\n",
        "    ranks = list(range(1, 8)) + ['J', 'Q', 'K']\n",
        "    if not (0 <= card_id < len(suits) * len(ranks)):\n",
        "        return \"1♠\"\n",
        "    suit_id, rank_id = divmod(card_id, len(ranks))\n",
        "    return f\"{ranks[rank_id]}{suits[suit_id]}\"\n",
        "\n",
        "\n",
        "class CardGameEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.action_space = spaces.Discrete(48)\n",
        "        self.observation_space = spaces.Box(0, 1, shape=(100,), dtype=np.float32)\n",
        "        self.deck = create_deck()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        random.shuffle(self.deck)\n",
        "        self.table = self.deck[:4]\n",
        "        self.player_hand = self.deck[4:7]\n",
        "        self.opponent_hand = self.deck[7:10]\n",
        "        self.remaining_deck = self.deck[10:]\n",
        "        self.player_captured = []\n",
        "        self.opponent_captured = []\n",
        "        self.turn = 0\n",
        "        self.game_over = False\n",
        "        self.player_abilities_used = [False]*4\n",
        "        return self._get_observation(), {}\n",
        "\n",
        "    def _get_observation(self):\n",
        "        obs = np.zeros(100, dtype=np.float32)\n",
        "        for c in self.player_hand:\n",
        "            cid = card_to_id(c)\n",
        "            if 0 <= cid < 44: obs[cid] = 1.0\n",
        "        for c in self.table:\n",
        "            cid = card_to_id(c)\n",
        "            if 0 <= cid < 44: obs[44 + cid] = 1.0\n",
        "        obs[88] = len(self.player_hand)/10\n",
        "        obs[89] = len(self.table)/10\n",
        "        obs[90] = len(self.player_captured)/20\n",
        "        obs[91] = self.turn/100\n",
        "        for i, used in enumerate(self.player_abilities_used):\n",
        "            obs[92+i] = 0.0 if used else 1.0\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.game_over:\n",
        "            return self._get_observation(), 0, True, False, {}\n",
        "        reward = 0\n",
        "        if action < 44:\n",
        "            reward = self._play_card_action(action)\n",
        "        else:\n",
        "            ability = action - 44\n",
        "            reward = self._use_ability_action(ability)\n",
        "        if not self.game_over:\n",
        "            self._opponent_turn()\n",
        "        if not self.player_hand and not self.opponent_hand and not self.remaining_deck:\n",
        "            self.game_over = True\n",
        "            winner = self._determine_winner()\n",
        "            reward += 10 if winner==\"win\" else -10 if winner==\"lose\" else 0\n",
        "        self._distribute_cards()\n",
        "        self.turn += 1\n",
        "        return self._get_observation(), reward, self.game_over, False, {}\n",
        "\n",
        "    def _play_card_action(self, card_id):\n",
        "        if card_id not in range(44): return -1\n",
        "        card = id_to_card(card_id)\n",
        "        if card not in self.player_hand: return -1\n",
        "        cv = card_value(card)\n",
        "        for tc in self.table[:]:\n",
        "            if card_value(tc) == cv:\n",
        "                self.player_hand.remove(card)\n",
        "                self.table.remove(tc)\n",
        "                self.player_captured += [card, tc]\n",
        "                reward = 2 + (3 if not self.table else 0)\n",
        "                return reward\n",
        "        self.player_hand.remove(card)\n",
        "        self.table.append(card)\n",
        "        return -0.1\n",
        "\n",
        "    def _use_ability_action(self, ability_id):\n",
        "        if ability_id >= 4 or self.player_abilities_used[ability_id]:\n",
        "            return -1\n",
        "        self.player_abilities_used[ability_id] = True\n",
        "        if ability_id == 0 and self.opponent_hand and self.player_hand:\n",
        "            opp_card = random.choice(self.opponent_hand)\n",
        "            my_card = random.choice(self.player_hand)\n",
        "            self.opponent_hand.remove(opp_card)\n",
        "            self.player_hand.remove(my_card)\n",
        "            self.opponent_hand.append(my_card)\n",
        "            self.player_hand.append(opp_card)\n",
        "            return 1\n",
        "        elif ability_id == 1 and self.table and self.player_hand:\n",
        "            table_card = random.choice(self.table)\n",
        "            my_card = random.choice(self.player_hand)\n",
        "            self.table.remove(table_card)\n",
        "            self.player_hand.remove(my_card)\n",
        "            self.table.append(my_card)\n",
        "            self.player_hand.append(table_card)\n",
        "            return 1\n",
        "        elif ability_id == 2:\n",
        "            return 0.5\n",
        "        elif ability_id == 3:\n",
        "            return 0.5 * sum(card_value(c) == 7 for c in self.player_hand)\n",
        "        return 0\n",
        "\n",
        "    def _opponent_turn(self):\n",
        "        if not self.opponent_hand:\n",
        "            return\n",
        "        for card in self.opponent_hand[:]:\n",
        "            cv = card_value(card)\n",
        "            for tc in self.table[:]:\n",
        "                if card_value(tc) == cv:\n",
        "                    self.opponent_hand.remove(card)\n",
        "                    self.table.remove(tc)\n",
        "                    self.opponent_captured += [card, tc]\n",
        "                    return\n",
        "        if self.opponent_hand:\n",
        "            self.table.append(self.opponent_hand.pop(0))\n",
        "\n",
        "    def _distribute_cards(self):\n",
        "        while len(self.player_hand) < 3 and self.remaining_deck:\n",
        "            self.player_hand.append(self.remaining_deck.pop(0))\n",
        "        while len(self.opponent_hand) < 3 and self.remaining_deck:\n",
        "            self.opponent_hand.append(self.remaining_deck.pop(0))\n",
        "\n",
        "    def _determine_winner(self):\n",
        "        p_points = (len(self.player_captured) > len(self.opponent_captured)) + (sum(card_value(c)==7 for c in self.player_captured) > sum(card_value(c)==7 for c in self.opponent_captured))\n",
        "        o_points = (len(self.opponent_captured) > len(self.player_captured)) + (sum(card_value(c)==7 for c in self.opponent_captured) > sum(card_value(c)==7 for c in self.player_captured))\n",
        "        if p_points > o_points: return \"win\"\n",
        "        elif o_points > p_points: return \"lose\"\n",
        "        return \"draw\"\n",
        "\n",
        "\n",
        "class QLearningPlayer:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.env = None\n",
        "\n",
        "    def train(self, total_timesteps=500):\n",
        "        self.env = CardGameEnv()\n",
        "        self.model = DQN('MlpPolicy', self.env, verbose=0, learning_rate=5e-4, buffer_size=100, learning_starts=10, batch_size=32, gamma=0.99, exploration_fraction=0.3, exploration_initial_eps=1.0, exploration_final_eps=0.1)\n",
        "        self.model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "    def evaluate(self, n_games=50):\n",
        "        if not self.model: return\n",
        "        return evaluate_policy(self.model, self.env, n_eval_episodes=n_games)\n",
        "\n",
        "    def play_game(self):\n",
        "        if not self.model: return \"Model not trained\"\n",
        "        obs, _ = self.env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action, _ = self.model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _, _ = self.env.step(action)\n",
        "        return self.env._determine_winner()\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        if self.model: self.model.save(filename)\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        self.model = DQN.load(filename)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    player = QLearningPlayer()\n",
        "    print(\"Training...\")\n",
        "    player.train(total_timesteps=100)\n",
        "    mean_reward, std_reward = player.evaluate(n_games=10)\n",
        "    print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n"
      ],
      "metadata": {
        "id": "0DaOfmLr6yZq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}